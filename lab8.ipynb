{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Language, Lab 8, Convergence to the Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simulation implements a simplified version of the language model from Kirby, Dowman & Griffiths (2007) using an explicit agent-based simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf')\n",
    "\n",
    "from math import log, log1p, exp\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "from numpy import mean # This is a handy function that calculate the average of a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Kirby, Dowman & Griffiths (2007), we assume a language is made up of a set of *variables*, each of which can exist in a number of different *variant* forms. This is a rather general characterisation that actually applies well to a number of linguistic phenomena. For example, we can think of the variables as different syntactic categories, and the variants as word orders. Alternatively, the variables could be verb-meanings and the variants different realisations of the past tense, and so on. Agents will produce (and learn from) data which simply exemplifies which variant they have for a particular variable (with the possibility of noise on transmission). We will group languages into two classes: regular languages (where the same variant is used for all variables) and irregular languages (where more than one variant is used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = 2           # The number of different variables in the language\n",
    "variants = 2            # The number of different variants each variable can take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for dealing with log probabilities\n",
    "\n",
    "Here are our standard functions for dealing with logs, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_subtract(x,y):\n",
    "    return x + log1p(-exp(y - x))\n",
    "\n",
    "def normalize_logprobs(logprobs):\n",
    "    logtotal = logsumexp(logprobs) #calculates the summed log probabilities\n",
    "    normedlogs = []\n",
    "    for logp in logprobs:\n",
    "        normedlogs.append(logp - logtotal) #normalise - subtracting in the log domain\n",
    "                                        #equivalent to dividing in the normal domain\n",
    "    return normedlogs\n",
    " \n",
    "def log_roulette_wheel(normedlogs):\n",
    "    r = log(random.random()) #generate a random number in [0,1), then convert to log\n",
    "    accumulator = normedlogs[0]\n",
    "    for i in range(len(normedlogs)):\n",
    "        if r < accumulator:\n",
    "            return i\n",
    "        accumulator = logsumexp([accumulator, normedlogs[i + 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to have a function that works a little like the `log_roulette_wheel` but instead always picks the most probable index (instead of picking indices proportional to their probability). We sometimes call this a \"winner take all\" function. While `log_roulette_wheel` can be used to implement *sampling*, `wta` can be used to implement *MAP* hypothesis selection. If there is more than one winner, then we pick randomly among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wta(probs):\n",
    "    maxprob = max(probs) # Find the maximum probability (works if these are logs or not)\n",
    "    candidates = []\n",
    "    for i in range(len(probs)):\n",
    "        if probs[i] == maxprob:\n",
    "            candidates.append(i) # Make a list of all the indices with that maximum probability\n",
    "    return random.choice(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce(language, log_error_probability):\n",
    "    variable = random.randrange(len(language)) # Pick a variant to produce\n",
    "    correct_variant = language[variable]\n",
    "    if log(random.random()) > log_error_probability:\n",
    "        return variable, correct_variant # Return the variable, variant pair\n",
    "    else:\n",
    "        possible_error_variants = list(range(variants))\n",
    "        possible_error_variants.remove(correct_variant)\n",
    "        error_variant = random.choice(possible_error_variants)\n",
    "        return variable, error_variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function produce takes a language, selects a random variable, and produces the relevant variant from the language, with a certain probability of error.\n",
    "\n",
    "- By looking at this code, can you tell how languages are represented in the simulation?\n",
    "- Can you see how errors on production work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying languages\n",
    "\n",
    "In this language model, prior probability is determined by language class: regular languages differ from irregular languages in their prior probability, and ultimately we are interested in the proportion of our simulated population who use regular languages. We therefore need a function to take a language and classify it as regular or not - the function `regular` does this.\n",
    "\n",
    "At this point, I should highlight a bit of a problematic bit of terminology that is used in the field. We've already seen the word \"regular\" in this course with our beta-binomial simulations, referring to whether or not something is given a variable label or a regular one. We're not using \"regular\" in that sense here. Instead we're using it in the same way linguists refer to \"regular verbs\" or \"irregular verbs\", for example. Regular verbs in English are ones that use a general strategy for forming the past tense (for example: *walk/walked*) whereas irregular verbs are ones that use an idiosyncratic strategy for forming the past tense (for example: *go/went*). Our language is \"regular\" in this sense if it uses the same strategy for each of its meanings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular(language):\n",
    "    first_variant = language[0]\n",
    "    for variant in language:\n",
    "        if variant != first_variant:\n",
    "            return False # The language can only be regular if every variant is the same as the first\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayesian bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprior(language, log_bias):\n",
    "    if regular(language):\n",
    "        number_of_regular_languages = variants\n",
    "        return log_bias - log(number_of_regular_languages) #subtracting logs = dividing\n",
    "    else:\n",
    "        number_of_irregular_languages = variants ** variables - variants # the double star here means raise to the power\n",
    "                                                                         # e.g. 4 ** 2 is four squared\n",
    "        return log_subtract(0, log_bias) - log(number_of_irregular_languages)\n",
    "        # log(1) is 0, so log_subtract(0, bias) is equivalent to (1 - bias) in the\n",
    "        # non-log domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `logprior` returns the prior probability (as a log probability) of a particular language. The strength of preference for regular languages is given as the second argument - if bias is over 0.5 (when converted back from a log probability), regular languages have higher prior probability.\n",
    "\n",
    "- Why are we dividing the bias by the number of regular and irregular languages in this function? Check you understand how these numbers are calculated.\n",
    "- How does this function differ from the prior from the Kirby, Dowman & Griffiths (2007) paper? (Hint: consider the case of more than two variables.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(data, language, log_error_probability):\n",
    "    loglikelihoods = []\n",
    "    logp_correct = log_subtract(0, log_error_probability) #probability of producing correct form\n",
    "    logp_incorrect = log_error_probability - log(variants - 1) #logprob of each incorrect variant\n",
    "    for utterance in data:\n",
    "        variable = utterance[0]\n",
    "        variant = utterance[1]\n",
    "        if variant == language[variable]:\n",
    "            loglikelihoods.append(logp_correct)\n",
    "        else:\n",
    "            loglikelihoods.append(logp_incorrect)\n",
    "    return sum(loglikelihoods) #summing log likelihoods = multiplying likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `loglikelihood` takes a language and a list of data and works out the (log) likelihood of the data given the language. We allows some small probability (given by the third argument) that a speaker will produce the ‘wrong’ variant, i.e. a variant other than that specified by their language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Bayesian learners calculate the posterior probability of each language based on some data, then select a language (‘learn’) based on those posterior probabilities. `learn` implements this. As discussed in the lecture, there are two ways you could select a language based on the posterior probability distribution:\n",
    "- You could pick the best language - i.e. the language with the highest posterior probability. This is called MAP (“maximum a posteriori”) learning.\n",
    "- Alternatively, you could pick a language probabilistically based on its posterior probability, without necessarily going for the best one every time (e.g. if language 0 has twice the posterior probability of language 1, you are twice as likely to pick it). This is called sampling (for “sampling from the posterior distribution”).\n",
    "\n",
    "The next bits of code implements both these ways of learning, using the `wta` function to do MAP learning and using `log_roulette_wheel` to do sampling (from previous labs, which assumed learners sample from the posterior). `all_languages` enumerates all possible languages for expressing `variables` variables and `variants` variants using a cute recursive method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_languages(variables, variants):\n",
    "    if variables == 0:\n",
    "        return [[]] # The list of all languages with zero variables is just one language, and that's empty\n",
    "    else:\n",
    "        result = [] # If we are looking for a list of languages with more than zero variables, \n",
    "                    # then we'll need to build a list\n",
    "        smaller_langs = all_languages(variables - 1, variants) # Let's first find all the languages with one \n",
    "                                                               # fewer variables\n",
    "        for language in smaller_langs: # For each of these smaller languages, we're going to have to create a more\n",
    "                                       # complex language by adding each of the possible variants\n",
    "            for variant in range(variants):\n",
    "                result.append(language + [variant])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don’t worry too much if you can’t figure out how it works, but you might get an idea if you figure out what steps it would take when called with different arguments, like `all_languages(0, 2)`, `all_languages(1, 2)`, `all_languages(2, 2)`, `all_languages(0, 3)`, `all_languages(1, 3)` and so on. Try that out in the cell below. You should also now be able to see if you were right with your answer to the question above as to how languages are represented in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `learn` implements hypothesis selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(data, log_bias, log_error_probability, learning_type):\n",
    "    list_of_all_languages = all_languages(variables, variants) # uses the parameters we set above\n",
    "    list_of_posteriors = []\n",
    "    for language in list_of_all_languages:\n",
    "        this_language_posterior = loglikelihood(data, language, log_error_probability) + logprior(language, log_bias)\n",
    "        list_of_posteriors.append(this_language_posterior)\n",
    "    if learning_type == 'map':\n",
    "        map_language_index = wta(list_of_posteriors) # For MAP learning, we pick the best language\n",
    "        map_language = list_of_all_languages[map_language_index]\n",
    "        return map_language\n",
    "    if learning_type == 'sample':\n",
    "        normalized_posteriors = normalize_logprobs(list_of_posteriors)\n",
    "        sampled_language_index = log_roulette_wheel(normalized_posteriors) # For sampling, we use the roulette wheel\n",
    "        sampled_language = list_of_all_languages[sampled_language_index]\n",
    "        return sampled_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterated learning\n",
    "\n",
    "`iterate`, is the top-level function which actually runs the simulation. It starts with a random language from the set of possible languages, and then each generation a learner learns from data produced by the previous agent. This function returns a list indicating whether the language is regular or not each generation. For convenience we encode the regular languages as `1` in this list and `0` otherwise. It also returns a second list, showing what each language was per generation in case you want more details. (Generally, we're just going to be using the first list for plotting graphs etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(generations, bottleneck, log_bias, log_error_probability, learning_type):\n",
    "    language = random.choice(all_languages(variables, variants))\n",
    "    if regular(language):\n",
    "        accumulator = [1]\n",
    "    else:\n",
    "        accumulator = [0]\n",
    "    language_accumulator = [language]\n",
    "    for generation in range(generations):\n",
    "        data = []\n",
    "        for i in range(bottleneck):\n",
    "            data.append(produce(language, log_error_probability))\n",
    "        language = learn(data, log_bias, log_error_probability, learning_type)\n",
    "        if regular(language):\n",
    "            accumulator.append(1)\n",
    "        else:\n",
    "            accumulator.append(0)\n",
    "        language_accumulator.append(language)\n",
    "    return accumulator, language_accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for example, to do an iterated learning simulation for 100 generations, a bottleneck of 5, with a bias in favour of regularity of 0.6, an error probability of 0.05, with sampling as the learning type, you'd use:\n",
    "\n",
    "```\n",
    "iterate(1000, 5, log(0.6), log(0.05), 'sample')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "**Note:** To answer some of these questions, you're going to have to run quite long simulation runs (up to 100000 generations to get super accurate results). In general, you probably want to keep the bottleneck values between 1 and 10.\n",
    "\n",
    "1. Using the default parameters suggested above, can you demonstrate convergence to the prior? You may want to start with a line graph of the results to get a sense of what's going on in the simulation. You could also plot a histogram to get the overall proportion of regular languages in the whole run, but notice that this is just the same as the average of your whole list so you don't really need a graph for this! You can get the average directly by using, for example, `print(mean(data[0]))` if `data` is the result of your simulation run (remember the `iterate` function returns whether the language at the each generation is regular or not as the first part of the data it returns, which is why we use `data[0]`.\n",
    "\n",
    "2. How does changing the error rate and bottleneck size affect the results for the sample learner? Make sure you run the simulation long enough to get repeatable results.\n",
    "\n",
    "3. Switch the learning type to MAP learning, and rerun the simulation. Can you show the amplification of prior bias that is shown in the paper?\n",
    "\n",
    "4. How is bias amplification affected by the bottleneck size? Can you run a range of simulations for different bottleneck sizes, find the means, and plot these nicely in a graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
